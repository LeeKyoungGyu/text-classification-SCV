{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43afc812",
   "metadata": {},
   "source": [
    "## 0. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3152a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "from soynlp.normalizer import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8008bd",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90884df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 피아노 좀 쳐봐.\\n싫어.\\n왜 손가락 없다고 유세 떠는 거야?\\n이씨.\\n비행...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>요즘에는 액수가 작네?\\n미안해 요즘에 용돈이 작아\\n그게 나랑 무슨 상관이야?\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>야 이거봐 완전 길동이 닯음\\n 진짜네 \\n야 그러지마.\\n왜 똑같구만 원숭이 \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>어이 신병.\\n이병 김범례.\\n와봐.\\n네.\\n네? 뒤질래?\\n.자.잘못들었습니다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>오늘 회의 안건인 길동프로그램의 출연자는 누구로 할 것인가에 대해 모두 의견 내주시...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>야 열심히들 해라 새끼들아\\n넵 감사해요 부장님\\n야 내가 언제 너 한테 말했냐\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>야 신입 여기 와 봐.\\n네?\\n이게 선배가 부르는데 말꼬리 올려? 너 커피 탈 줄...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            class                                       conversation\n",
       "idx                                                                 \n",
       "1           일반 대화  오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....\n",
       "2           일반 대화  오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....\n",
       "3       기타 괴롭힘 대화  너 피아노 좀 쳐봐.\\n싫어.\\n왜 손가락 없다고 유세 떠는 거야?\\n이씨.\\n비행...\n",
       "4           갈취 대화  요즘에는 액수가 작네?\\n미안해 요즘에 용돈이 작아\\n그게 나랑 무슨 상관이야?\\n...\n",
       "5       기타 괴롭힘 대화  야 이거봐 완전 길동이 닯음\\n 진짜네 \\n야 그러지마.\\n왜 똑같구만 원숭이 \\n...\n",
       "...           ...                                                ...\n",
       "4946  직장 내 괴롭힘 대화  어이 신병.\\n이병 김범례.\\n와봐.\\n네.\\n네? 뒤질래?\\n.자.잘못들었습니다?...\n",
       "4947  직장 내 괴롭힘 대화  오늘 회의 안건인 길동프로그램의 출연자는 누구로 할 것인가에 대해 모두 의견 내주시...\n",
       "4948  직장 내 괴롭힘 대화  야 열심히들 해라 새끼들아\\n넵 감사해요 부장님\\n야 내가 언제 너 한테 말했냐\\n...\n",
       "4949        일반 대화  오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....\n",
       "4950  직장 내 괴롭힘 대화  야 신입 여기 와 봐.\\n네?\\n이게 선배가 부르는데 말꼬리 올려? 너 커피 탈 줄...\n",
       "\n",
       "[4950 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path =\"~/aiffel/dktc/data2/train2.csv\"\n",
    "train_data = pd.read_csv(train_data_path,index_col=0)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c89928",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b41115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17276ee3",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비 (Data preparation)\n",
    "### 2.1-1 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67dc5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # synolp\n",
    "    emoticon_normalize(sentence)\n",
    "    repeat_normalize(sentence)\n",
    "    # base preprocess\n",
    "    sentence = re.sub(r'([^a-zA-Zㄱ-ㅎ가-힣?.!,])', \" \", sentence)\n",
    "    sentence = re.sub(r'!+', '!', sentence)\n",
    "    sentence = re.sub(r'\\?+', '?', sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 엔터 구분 (\\n)\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933ecf9",
   "metadata": {},
   "source": [
    "### 2.1-2 전처리 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6504c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4950/4950 [00:01<00:00, 3290.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# 학습할 문장이 담길 배열\n",
    "sentences = []\n",
    "\n",
    "for val in tqdm(train_data['conversation']):\n",
    "    sentences.append(preprocess_sentence(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbfbf3",
   "metadata": {},
   "source": [
    "### 2.2 최대 길이 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4732905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc3406",
   "metadata": {},
   "source": [
    "### 2.3 class(label) 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2deeb6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일반 대화']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASS_NAMES)\n",
    "\n",
    "train_data['class'] = encoder.transform(train_data['class'])\n",
    "labels = train_data['class']\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab6a0a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'협박 대화': 4, '갈취 대화': 0, '직장 내 괴롭힘 대화': 3, '기타 괴롭힘 대화': 1, '일반 대화': 2}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {class_name: encoder.transform([class_name])[0] for class_name in CLASS_NAMES}\n",
    "print(\"Class mapping:\", class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20060a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6058053d",
   "metadata": {},
   "source": [
    "### 2.4 train-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86c541e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef113adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data augmentation:  3960\n",
      "after data augmentation:  11880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>오늘 날씨 어때 ? 맑고 따뜻해 . 좋네 ! 주말에 계획 있어 ? 등산 갈 생각이야...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>사장님 . 배달이 분까지인데 분이나 늦었잖아요 . 죄송합니다 . 배달원한테 전달을 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>이번 달 월급 들어오면 갚는다며 . 들어오자마자 다 뜯겨서 그래 . 미안해 . 그 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>야 우리 내일 현장학습인 거 알지 ? 응 ? 알지 . 그럼 니가 내 대신 내도시락도...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>여기가 어딘가요 ? 야 있는 돈 다 내놔 . 맞기 싫으면 아니 댁은 누구신대 다짜고...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>야 일 제대로 안해 ? 아니 물량수준이 희망사항 수준인데 그래서 뭐 임마 그 많은 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>역시 난 니가 올 줄 알았어 . 우리가족을 인질로 잡아 ? 니가 그러고도 사람이야 ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>밤마다 뭘하는 지 시끄러워서 잠을 못자겠다고 ! 밤에 뭐 하긴 자지 . 자는 데 왜...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>오늘 날씨 어때 ? 맑고 따뜻해 . 좋네 ! 주말에 계획 있어 ? 등산 갈 생각이야...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>아 ! 운전 똑바로 하셔야죠 ! 죄송해요이걸 어쩌지 차 다 긁혔잖아요 ! 일단 보험...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11880 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  class\n",
       "idx                                                           \n",
       "3667  오늘 날씨 어때 ? 맑고 따뜻해 . 좋네 ! 주말에 계획 있어 ? 등산 갈 생각이야...      2\n",
       "1021  사장님 . 배달이 분까지인데 분이나 늦었잖아요 . 죄송합니다 . 배달원한테 전달을 ...      1\n",
       "3227  이번 달 월급 들어오면 갚는다며 . 들어오자마자 다 뜯겨서 그래 . 미안해 . 그 ...      0\n",
       "1030  야 우리 내일 현장학습인 거 알지 ? 응 ? 알지 . 그럼 니가 내 대신 내도시락도...      1\n",
       "2909  여기가 어딘가요 ? 야 있는 돈 다 내놔 . 맞기 싫으면 아니 댁은 누구신대 다짜고...      0\n",
       "...                                                 ...    ...\n",
       "4427  야 일 제대로 안해 ? 아니 물량수준이 희망사항 수준인데 그래서 뭐 임마 그 많은 ...      3\n",
       "467   역시 난 니가 올 줄 알았어 . 우리가족을 인질로 잡아 ? 니가 그러고도 사람이야 ...      4\n",
       "3093  밤마다 뭘하는 지 시끄러워서 잠을 못자겠다고 ! 밤에 뭐 하긴 자지 . 자는 데 왜...      4\n",
       "3773  오늘 날씨 어때 ? 맑고 따뜻해 . 좋네 ! 주말에 계획 있어 ? 등산 갈 생각이야...      2\n",
       "861   아 ! 운전 똑바로 하셔야죠 ! 죄송해요이걸 어쩌지 차 다 긁혔잖아요 ! 일단 보험...      0\n",
       "\n",
       "[11880 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### train data 증강\n",
    "\n",
    "# 데이터 증강 함수\n",
    "\n",
    "def random_deletion(words, p=0.3):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words) - 1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return \"\".join(new_words)\n",
    "\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = (\n",
    "        new_words[random_idx_2],\n",
    "        new_words[random_idx_1],\n",
    "    )\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def random_swap(words, n=3):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "print(\"before data augmentation: \", len(train_sentences))\n",
    "\n",
    "train_splted = pd.DataFrame({ \"sentence\": train_sentences, \"class\": train_labels })\n",
    "\n",
    "# random deletion\n",
    "train_splted_rd = train_splted.copy()\n",
    "train_splted_rd[\"sentence\"] = train_splted_rd[\"sentence\"].apply(random_deletion)\n",
    "\n",
    "# random swap\n",
    "train_splted_rs = train_splted.copy()\n",
    "#train_splted_rs[\"sentence\"] = random_swap(train_splted_rs[\"sentence\"])\n",
    "\n",
    "# with data augmentation\n",
    "train_concated = pd.concat([train_splted , train_splted_rd , train_splted_rs])\n",
    "\n",
    "print(\"after data augmentation: \", len(train_concated))\n",
    "\n",
    "train_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "437ac033",
   "metadata": {},
   "source": [
    "## 3. 모델\n",
    "### 3.1-1 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d91b5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 토크나이저와 모델 준비\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d06ff5",
   "metadata": {},
   "source": [
    "### 3.1-2 토크나이저 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bbbfc945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 BERT 입력 형식으로 변환\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True, max_length=MAX_LEN) # 뒤쪽에 패딩\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, padding=True, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12091035",
   "metadata": {},
   "source": [
    "### 3.2 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6b767b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d702cc",
   "metadata": {},
   "source": [
    "### 3.3 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7fd3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "lr = 5e-5\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf01fe",
   "metadata": {},
   "source": [
    "### 3.4 TF 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27a4bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 데이터셋 생성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).shuffle(100).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    ")).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b82a47",
   "metadata": {},
   "source": [
    "### 3.5 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a1443c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "             run_eagerly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1f459",
   "metadata": {},
   "source": [
    "### 3.6 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c7cb9",
   "metadata": {},
   "source": [
    "### 3.6-1 콜백 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4744341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # 검증 손실을 모니터링\n",
    "    patience=3,            # 3 에포크 동안 개선되지 않으면 중지\n",
    "    restore_best_weights=True  # 최상의 가중치를 복원\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_2.weights.h5',  # 모델 가중치를 저장할 파일 경로\n",
    "    monitor='val_loss',        # 검증 손실을 모니터링\n",
    "    save_best_only=True,       # 최상의 모델만 저장\n",
    "    save_weights_only=True,   # 저장 (가중치)\n",
    "    mode='min',                # 'val_loss'가 최소일 때 저장\n",
    "    verbose=1                  # 저장 시 로그 출력\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb28f6",
   "metadata": {},
   "source": [
    "### 3.6-2 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46d34bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "495/495 [==============================] - 226s 429ms/step - loss: 1.5570 - accuracy: 0.2424 - val_loss: 1.6075 - val_accuracy: 0.2253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60748, saving model to best_model_2.weights.h5\n",
      "Epoch 2/10\n",
      "495/495 [==============================] - 210s 423ms/step - loss: 1.6163 - accuracy: 0.2015 - val_loss: 1.6087 - val_accuracy: 0.2253\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.60748\n",
      "Epoch 3/10\n",
      "495/495 [==============================] - 209s 423ms/step - loss: 1.6136 - accuracy: 0.2144 - val_loss: 1.6114 - val_accuracy: 0.2253\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.60748\n",
      "Epoch 4/10\n",
      "495/495 [==============================] - 209s 422ms/step - loss: 1.6133 - accuracy: 0.2035 - val_loss: 1.6092 - val_accuracy: 0.2253\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.60748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7990b4569d90>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCH,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af49aa",
   "metadata": {},
   "source": [
    "### 3.7 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "892a97c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 15s 121ms/step - loss: 1.6075 - accuracy: 0.2253\n",
      "평가 결과: [1.6074848175048828, 0.22525252401828766]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "evaluation = model.evaluate(val_dataset)\n",
    "print(\"평가 결과:\", evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de59ba",
   "metadata": {},
   "source": [
    "## 4. 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a41826a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/aiffel/dktc/data2/test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8270/1310437528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"~/aiffel/dktc/data2/test.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/aiffel/dktc/data2/test.json'"
     ]
    }
   ],
   "source": [
    "test_data_path = \"~/aiffel/dktc/data2/test.json\"\n",
    "\n",
    "with open(test_data_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    test = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27d282e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12093/2082288350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     test_predictions = model.predict({\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2377\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2378\u001b[0m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_predicst = list()\n",
    "\n",
    "for key in test:\n",
    "    test_sentence = test['text']\n",
    "    \n",
    "    test_encodings = tokenizer(test_sentence, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "    \n",
    "    test_predictions = model.predict({\n",
    "        \"input_ids\": test_encodings[\"input_ids\"],\n",
    "        \"token_type_ids\": test_encodings[\"token_type_ids\"],\n",
    "        \"attention_mask\": test_encodings[\"attention_mask\"]\n",
    "    }) # [ 0.7805823,  2.6188664, -2.0281641, -0.9672525]\n",
    "    test_class_probabilities = tf.nn.softmax(test_predictions.logits, axis=-1).numpy() # [[0.13297564 0.8358507  0.00801584 0.02315779]]\n",
    "    test_predicted_class = np.argmax(test_class_probabilities, axis=1) # [ 1 ]\n",
    "    test_predicst.append(test_predicted_class[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c75fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelnum_to_text(x):\n",
    "    if x == 1 : # 갈취\n",
    "        return '01'\n",
    "    if x == 2 : # 직장\n",
    "        return '02'\n",
    "    if x == 3 : # 기타\n",
    "        return '03'\n",
    "    if x == 0 : # 협박 \n",
    "        return '00'\n",
    "    \n",
    "submission = pd.DataFrame({'class':test_predicst}, index=list(test.keys()))\n",
    "\n",
    "submission['class'] = submission['class'].apply(labelnum_to_text)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ce830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  submission.to_csv('~/aiffel/dktc/sub/base_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0297f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf97c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ce239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f12ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
